# 内存

## 缓存

### 缓存提升速度原理

1. Cache缩短了CPU的等待时间。

2. Cache的工作原理是保存CPU最常用数据；当Cache中保存着CPU要读写的数据时，CPU直接访问Cache。由于Cache的速度与CPU相当，CPU就能在零等待状态下迅速地实现数据存取。

3. 只有在Cache中不含有CPU所需的数据时CPU才去访问主存。Cache在CPU的读取期间依照优化命中原则淘汰和更新数据，可以把Cache看成是主存与CPU 之间的缓冲适配器，借助于Cache，可以高效地完成DRAM内存和CPU之间的速度匹配。

### cache line 缓存行

缓存行越大，局部空间效率越高,读取时间越慢!

缓存行越小，局部空间效率越低，读取时间越快!

> 总所周知，计算机将数据从主存读入Cache时，是把要读取数据附近的一部分数据都读取进来
>  这样一次读取的一组数据就叫做`CacheLine`，每一级缓存中都能放很多的`CacheLine`

### 缓存的映射策略

- **直接映射**（Direct Mapped）：一个内存地址的数据只能缓存到一个固定的cache line上。为减少冲突，提高命中率，要求对内存的访问需比较平均，但实际很少有这样的使用场景。
- **全相联**（Fully Associative）：任何一个内存地址的数据可缓存到任意cache line上，搜索效率可能会受限。适用于小容量的缓存空间或对效率要求不高的场景（软件层可能不同）。
- **组相联**（N-Way Associative）：按**组**（Set）等分缓存空间，一个内存地址唯一确定一个组，数据则可以缓存到组内的任意cache line。为以上两种的组合，是通常使用的策略。

### 缓存写入

#### write-through cache

写入缓存的时候,同时写入内存

#### write-back cache

只更新缓存,只有在数据被迫离开缓存时,才写入内存中

#### write-buffer

只要数据一进入buffer中,就立即缓存,而不是等待完全的延迟将数据写入内存中

### 未命中率 miss-rate

#### 三C模型

##### 强制性未命中(compulsory miss)

又称为冷启动未命中，因为缓存一开始是空的，这是对项目的第一次引用。

##### 容量未命中(capacity miss)

缓存的空间不足而不得不踢出一个项目以将新项目引入缓存。

##### 冲突未命中(conflict miss)

这种未命中出现在硬件中，因为硬件缓存中对项的放置位置有限制，这是由于所谓的集合关联性(cache associativity)。它不会出现在操作系统页面缓存中，因为这样的缓存总是完全关联的，即对页面放置的内存位置没有限制。

> 冲突未命中（conflict miss) 和缓存的实现方式有关。大多数缓存，尤其是硬件缓存，由于它们需要设计的较为简单，因此限制了**块**可以被放置的位置。例如 block i 只能放在 block (**i mod cache size**) 的地方，以上图为例，缓存的大小为4，如果要取 block 8的数据，则只能放在缓存的第0块，同样，block 9 放在缓存的 block  1处。因此，如果要取的块为 block 0， block 4，block 8，那么计算出来都应该放在缓存 block 0的位置，因此放  block 4时，会覆盖原来 block 0的数据，加入需要循环的访问 block 0，block 8，block 0，block  8，这样就会一直不能命中，即使缓存有多余的空间，但位置的限制导致一直覆盖缓存上 block 0 的数据，造成**冲突未命中**。![image-20220830195516083](E:\笔记\计算机体系结构.assets\image-20220830195516083.png)

##### coherency miss 一致性

缓存刷新引起的未命中

多线程以及多核增加了缓存的复杂性,引入第四个C coherency  
缓存刷新以保证多处理器中多个缓存一致

### 内存平均访问时间

![image-20220902115544778](E:\笔记\计算机体系结构.assets\image-20220902115544778.png)

hit time: 击中缓存所耗费的时间

miss penalty: 

### 六种基础缓存优化

#### Larger block size to reduce miss rate 增大块的大小来降低未命中率

利用空间局部性,减少了强制未命中,但是也增大了未命中的成本,因为更大的块会减少内存中tags的序号,tags的序号会轻微的降低静态损耗.太大的块大小也会增加容量未命中,以及冲突未命中,特别是在小的缓存中,要选择合适的块大小

> tag是内存地址的高位部分，存储在cache中用于标识相应的数据。一般来说cache的大小指的是所保存的数据规模，并不包括其中需要储存tag的物理空间

#### Bigger caches to reduce miss rate 更大的缓存来减低未命中率

缺点是 可能在大缓存中带来更长的hit time,更高的未命中成本,以及 能源的损耗.同时,更大的缓存会增加静态损耗和动态损耗.

#### Higher associativity to reduce miss rate 更高的缓存关联性来降低未命中率

增加associativity可以减少冲突未命中,当然associativity太高也会带来更长的hit time,同时也会带来能源的损耗

> cache associativity
> 
> ![image-20220902191154681](E:\笔记\计算机体系结构.assets\image-20220902191154681.png)

#### Multilevel caches to reduce miss penalty 多级缓存减少未命中的成本

减少hit time?还是增大缓存减少主存与处理器的速度差距?使用多级缓存可以简化这个问题.

第一级缓存足够小以匹配处理器的时钟速率,第二或第三级缓存足够大以捕获许多访问主存的请求.多级缓存的能源利用率高于单个总缓存.如果使用L1,L2代表第一,第二级缓存的话,可以重新定义内存评价访问时间:![image-20220902200132101](E:\笔记\计算机体系结构.assets\image-20220902200132101.png)

#### Giving priority to read misses over writes to reduce miss penalty  优先考虑读未命中而不是写未命中来降低未命中的损失

使用write buffer来实现这种优化,使用write buffer会产生风险,因为write buffer保存着最新的数据,这可能发生读未命中所需要的,这就叫做read-after-write.

一种解决方案是,在读未命中时,检查write buffer内的数据,如果没有冲突,并且内存系统正常工作,在写之前,发送读到的内容以降低未命中的成本.大多数处理器都给予读相比于写更高的优先级,这个决定只对能源有一点的损耗.

#### Avoiding address translation during indexing of the cache to reduce hit time 在索引缓存时避免地址换以降低hit time

缓存必须将来自处理器的虚拟地址转换为物理地址以访问内存.一个常见的优化是使用页偏移(the page offset)---虚拟地址和物理地址公共的部分,以此来索引缓存.

缺点: 会为系统引入更多的复杂性,并且会限制L1缓存的大小和结构.  
优点: 在关键路径移除了TLB的访问,好处大于坏处.

## 构建缓存以及主存的技术

#### access time 访问时间

一个请求读取到返回的时间

#### cycle time 循环时间

一个不相干的请求的最短时间

### SRAM

### DRAM

### Flash
